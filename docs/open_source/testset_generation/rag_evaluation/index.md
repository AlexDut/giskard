# ü•á RAGET Evaluation

> ‚ö†Ô∏è **RAGET is currently in early version and is subject to change**. Feel free to reach out on our [Discord server](https://discord.gg/fkv7CAr3FE) if you have any trouble or to provide feedback.


After automatically generating a test set for your RAG agent using RAGET, you can then evaluate the **correctness 
of the agent's answers** compared to the reference answers (using a LLM-as-a-judge approach). The main purpose
of this evaluation is to help you **identify the weakest components in your RAG agent**.



## Correctness Evaluation on the Generated Test Set

To evaluate, use the `giskard.rag.evaluate` function. You can also [pass custom metrics](custom_metrics) to the 
`evaluate` function to compute custom scores.

```python
from giskard.rag import evaluate

def answer_fn(question, history=None):
    # a function that wraps your model predictions function
    messages = history if history else []
    message.append({"role": "user", "content": question})

    return your_agent.predict(messages)

report = evaluate(answers_fn, testset=testset, knoledge_base=knowledge_base)
report
```

The evaluation generates a {class}`~giskard.rag.RAGReport` object. This report is an HTML widget presenting all the 
results of the evaluation:

![image](../../../_static/rag_report.png)

It displays automatically in a notebook, but you can save it and view it in any browser: 
`report.save_html("report.html")`.

### RAG Components Scores
RAGET computes scores for each component of the RAG agent. The scores are computed by aggregating the correctness 
of the agent's answers on different question types (see question type to component mapping [here](q_types)). 
Each score grades a component on a scale from 0 to 100, 100 being a perfect score. **Low scores can help you identify 
weaknesses of your RAG agent and which components need improvement.**

Here is the list of components evaluated with RAGET:
- **`Generator`**: the LLM used inside the RAG to generate the answers
- **`Retriever`**: fetch relevant documents from the knowledge base according to a user query
- **`Rewriter`** (optional): rewrite the user query to make it more relevant to the knowledge base or to account for chat history
- **`Router`** (optional): filter the query of the user based on his intentions (intentions detection)
- **`Knowledge Base`**: the set of documents given to the RAG to generate the answers


### Analyze Correctness and Failures
All the information contained in the report can also be accessed through the API:
    
```python
report.to_pandas()
```

You can access the correctness of the agent aggregated in various ways or analyze only it failures: 

```python
# Correctness on each topic of the Knowledge Base
report.correctness_by_topic()

# Correctness on each type of question
report.correctness_by_question_type()

# get all the failed questions
report.failures

# get the failed questions filtered by topic and question type
report.get_failures(topic="Topic from your knowledge base", question_type="simple")
```


(custom_metrics)=
### Evaluate your RAG with custom metrics
**You can pass additional evaluation metrics to the `evaluate` function**. They will be computed during the evaluation. 
We currently provide [RAGAS metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html) as additional metrics, 
but you can use your own metric as well. Just wrap your metric inside a `giskard.rag.metric.Metric` object and 
implement a `__call__` function. Then you can pass it to the `evaluate` function as follows: 

```python
import pandas as pd
from giskard.rag.metric import Metric

your_metric_fn = ...

class MyCustomMetric(Metric):
    def __call__(self, testset, answers, *args, **kwargs):
        # your custom metric implementation
        metric_df = pd.DataFrame([{"id": q.id, "value": your_metric_fn(q, a)} for q, a in zip(testset.itertuples(), answers)])
        return {self.name: metric_df.set_index("id")}

your_custom_metric = MyCustomMetric(name="My custom metric")
report = evaluate(answers_fn, testset=testset, knowledge_base=knowledge_base, additional_metrics=[your_custom_metric])
```

The results of your metrics will be displayed in the report object as histograms and will be available inside the report main `DataFrame`. 

To use RAGAS metrics, make sure that the `ragas` package is installed in your environment. You can install it with `pip install ragas`.

```python
from giskard.rag.metrics.ragas_metrics import ragas_context_recall, ragas_faithfulness

report = evaluate(answers_fn, testset=testset, knowledge_base=knowledge_base, additional_metrics=[ragas_context_recall, ragas_faithfulness])
```

## Going Further: Giskard's Visual Interface 

The tests generated by RAGET integrate directly with the **Giskard Hub** to allow for collaboration on the curation, 
review and execution of tests.

### Step 1: Convert the test set into a test suite
Let's convert our test set into an actionable test suite ({class}`giskard.Suite`) that we can save and reuse in further iterations.

```python
test_suite = testset.to_test_suite("My first test suite")

test_suite.run(model=giskard_model)
```

![](./test_suite_widget.png)


Note that you can split the test suite on the question metadata values, for instance on each question type. 

```python
test_suite_by_question_types = testset.to_test_suite("Split test suite", slicing_metadata=["question_type"])
```

Jump to the [test customization](https://docs.giskard.ai/en/latest/open_source/customize_tests/index.html) and [test integration](https://docs.giskard.ai/en/latest/open_source/integrate_tests/index.html) sections to find out everything you can do with test suites.

### Step 2: Wrap your model
Before evaluating your model with a test suite, you must wrap it as a `giskard.Model`. This step is necessary to ensure a common format for your model and its metadata. You can wrap anything as long as you can represent it in a Python function (for example an API call to Azure or OpenAI). We also have pre-built wrappers for LangChain objects, or you can create your own wrapper by extending the `giskard.Model` class if you need to wrap a complex object such as a custom-made RAG communicating with a vectorstore.

To do so, you can follow the instructions from the [LLM Scan feature](../scan/scan_llm/index.md#step-1-wrap-your-model). Make sure that you pass `feature_names = "question"` when wrapping your model, so that it matches the question column of the test set.

Detailed examples can also be found on our {doc}`LLM tutorials section </tutorials/llm_tutorials/index>`.

Once you have wrapped your model, we can proceed with evaluation.

```python
test_suite.run(model=giskard_model)
```

### Step 3: upload your test suite to the Giskard Hub

Uploading a test suite to the hub allows you to:
* Compare the quality of different models and prompts to decide which one to promote
* Create more tests relevant to your use case, combining input prompts that make your model fail and custome evaluation criteria
* Share results, and collaborate with your team to integrate business feedback

To upload your test suite, you must have created a project on Giskard Hub and instantiated a Giskard Python client. If you haven't done this yet, follow the first steps of [upload your object](https://docs.giskard.ai/en/latest/giskard_hub/upload/index.html#upload-your-object) guide. 

Then, upload your test suite and model like this:
```python
test_suite.upload(giskard_client, project_id)  # project_id should be the id of the Giskard project in which you want to upload your suite
giskard_model.upload(giskard_client, project_id)
```

> ‚ö†Ô∏è To upload your model to the hub, it must be pickleable. If your model is not, you must extend the `giskard.Model` class and override the `save_model` and `load_model` methods to properly save and load the non-pickleable parts of your model (e.g. the vector store). You can find an [example here](../scan/scan_llm/index.md#step-1-wrap-your-model) inside the "Wrap a custom RAG" tab.

[Here's a demo](https://huggingface.co/spaces/giskardai/giskard) of the Giskard Hub in action.



## Troubleshooting
If you encounter any issues, join our [Discord community](https://discord.gg/fkv7CAr3FE) and ask questions in our #support channel.