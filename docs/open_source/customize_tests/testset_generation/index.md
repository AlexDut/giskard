# ðŸ§° RAG toolset

Retrieval Augmented Generative models (RAGs) combine LLM models and data sources to produce domain-specific language models able to 
answer precise questions whose answer are available inside a knowledge base. These models are often extremely specialized to a use-case
defined by the information present inside the knowledge base. The specialization of the model makes generic evaluations irrelevant to verify 
the model's behavior (e.g. hallucinations, trustworthiness, etc.). To this end, the Giskard python library provides a toolset dedicated to RAG models 
that generates question/answer pairs from the knowledge base of the model.

## How does it work?

The automatic testset generation explores the Knowledge Base (KB) of your model and generate questions and answers related to specific topics 
available inside the KB. Specifically, a topic from the KB is selected at random, then the related excerpts from the KB are extracted to create 
a `reference_context`. Then we generate a `question` along with a `reference_answer` using an LLM (specifically, we use **OpenAI GPT-4**). 

The generated testset contains a list of questions specific to the model's knowledge base. The model should theoretically answer all these 
questions correctly. Yet, hallucination or imprecise answers can be generated by the model. This testset allows to evaluate how frequent 
these undesired behaviors happen.

### What data are being sent to OpenAI/Azure OpenAI

In order to perform LLM-assisted detectors, we will be sending the following information to OpenAI/Azure OpenAI:

- Data provided in your Dataset
- Text generated by your model
- Model name and description

### Will the testset generation work in any language?

The testset quality depends on GPT-4 capabilities regarding your model's language.

## Before starting

Before starting, make sure you have installed the LLM flavor of Giskard:

```bash
pip install "giskard[llm]"
```

For the LLM-assisted detectors to work, you need to have an OpenAI API key. You can set it in your notebook
like this:

:::::::{tab-set}
::::::{tab-item} OpenAI

```python
import os

os.environ["OPENAI_API_KEY"] = "sk-â€¦"
```

::::::
::::::{tab-item} Azure OpenAI

Require `openai>=1.0.0`

```python
import os
from giskard.llm import set_llm_model

os.environ['AZURE_OPENAI_API_KEY'] = '...'
os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://xxx.openai.azure.com'
os.environ['OPENAI_API_VERSION'] = '2023-07-01-preview'


# You'll need to provide the name of the model that you've deployed
# Beware, the model provided must be capable of using function calls
set_llm_model('my-gpt-4-model')
```

::::::
:::::::

We are now ready to start.


## Step 1: Format and load your Knowledge Base
The RAG toolset currently only handles knowledge bases as pandas `DataFrame`. If the DataFrame has multiple columns,
they will be concatenated automatically. If only some of the columns contains the knowledge, you can specify it when building 
the generator by passing a list of column names to the `knowledge_base_features` argument.


```python
knowledge_base_df = pd.read_*("path/to/your/knowledge_base")
feature_names = ["col1", "col2"]
knowledge_base_df["page_content"] = knowledge_base_df[feature_names].apply(" ".join, axis=1)
```

## Step 2: Generate the testset
Once the knowledge base is loaded as a pandas `DataFrame`, you can generate the testset with the 
`KnowledgeBaseTestsetGenerator`. 


```python
from giskard.rag import KnowledgeBaseTestsetGenerator

generator = KnowledgeBaseTestsetGenerator(knowledge_base_df, 
                    model_name="Model name",
                    model_description="Description of the model",
                    knowledge_base_features=["page_content"])

testset = generator.generate_dataset(num_samples=10)
```

## Step 3: Evaluate your model
```python
from giskard.testing.tests.llm import test_llm_correctness

test_llm_correctness(model, testset, threshold=0.8).execute()
```

## What's next?

Your scan results may have highlighted important vulnerabilities. There are 2 important actions you can take next:

### 1. Generate a test suite from the testset:

Turn the generated testset into an actionable test that you can save and reuse in further iterations.

```python
test_suite = scan_results.generate_test_suite("My first test suite")

# You can run the test suite locally to verify that it reproduces the issues
test_suite.run()
```

Jump to the [test customization](https://docs.giskard.ai/en/latest/open_source/customize_tests/index.html) and [test integration](https://docs.giskard.ai/en/latest/open_source/integrate_tests/index.html) sections to find out everything you can do with test suites.

### 2. Upload your test suite to the Giskard Hub to:
* Compare the quality of different models and prompts to decide which one to promote
* Create more tests relevant to your use case, combining input prompts that make your model fail and custome evaluation criteria
* Share results, and collaborate with your team to integrate business feedback

To upload your test suite, you must have created a project on Giskard Hub and instantiated a Giskard Python client. If you haven't done this yet, follow the first steps of [upload your object](https://docs.giskard.ai/en/latest/giskard_hub/upload/index.html#upload-your-object) guide.

Then, upload your test suite like this:
```python
test_suite.upload(giskard_client, project_key)
```

[Here's a demo](https://huggingface.co/spaces/giskardai/giskard) of the Giskard Hub in action.

## Troubleshooting

If you encounter any issues, join our [Discord community](https://discord.gg/fkv7CAr3FE) and ask questions in our #support channel.
