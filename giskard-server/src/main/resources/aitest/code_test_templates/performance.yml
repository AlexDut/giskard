title: Performance
id: performance
order: 3
items:
  - id: auc
    title: AUC
    hint: Test if the AUC is high enough for a sub-population
    modelTypes:
      - MULTICLASS_CLASSIFICATION
      - BINARY_CLASSIFICATION
    # language=Python
    code: |-
      """
      Detailed description : Test if the model AUC performance is higher than a threshold for a given sub-population
      Examples : The test is passed when the AUC for women is higher than 0.7
      Inputs :
              - df_slice(pd.Dataframe) : sub-population of the test dataset selected during Test Suite Creation
              - model : model selected during Test Suite Creation
              - threshold : threshold value of AUC metrics
              - target : target column name
      Outputs :
              - total rows tested  : length of df_slice tested
              - metric :  the AUC performance metric
              - passed : TRUE if AUC metrics > threshold
      """
      tests.performance.test_auc(
          df_slice=test_df[:len(test_df) // 2],
          model=model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )

  - id: f1
    title: F1
    # language=Python
    code: |-
      tests.performance.test_f1(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: diff_f1
    title: F1 difference
    # language=Python
    code: |-
      tests.performance.test_diff_f1(
          test_df,
          model,
          filter_1=test_df[:len(test_df)//2].index,
          filter_2=test_df[len(test_df)//2:].index,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: accuracy
    title: Accuracy
    # language=Python
    code: |-
      tests.performance.test_accuracy(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: diff_accuracy
    title: Accuracy difference
    # language=Python
    code: |-
      tests.performance.test_diff_accuracy(
          test_df,
          model,
          filter_1=test_df[:len(test_df)//2].index,
          filter_2=test_df[len(test_df)//2:].index,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: precision
    title: Precision
    # language=Python
    code: |-
      tests.performance.test_precision(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: diff_precision
    title: Precision difference
    # language=Python
    code: |-
      tests.performance.test_diff_precision(
          test_df,
          model,
          filter_1=test_df[:len(test_df)//2].index,
          filter_2=test_df[len(test_df)//2:].index,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: recall
    title: Recall
    # language=Python
    code: |-
      tests.performance.test_recall(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: diff_recall
    title: Recall difference
    # language=Python
    code: |-
      tests.performance.test_diff_recall(
          test_df,
          model,
          filter_1=test_df[:len(test_df)//2].index,
          filter_2=test_df[len(test_df)//2:].index,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: neg_rmse
    title: Negative RMSE
    # language=Python
    code: |-
      tests.performance.test_neg_rmse(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: neg_mae
    title: Negative MAE
    # language=Python
    code: |-
      tests.performance.test_neg_mae(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
  - id: r2
    title: R2
    # language=Python
    code: |-
      tests.performance.test_r2(
          test_df,
          model,
          threshold=0.1,
          target='<TARGET COLUMN>'
      )
